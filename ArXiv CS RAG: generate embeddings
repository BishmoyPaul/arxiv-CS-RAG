{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9148815,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ğŸ”ğŸ“š ArXiv CS RAG ğŸ“šğŸ”\n\nThis is the code for generating embeddings for [ArXiv CS RAG](https://huggingface.co/spaces/bishmoy/Arxiv-CS-RAG), a Huggingface space for searching paper embeddings and querying using large language models (LLMs) of your choice. This code takes advantage of the amazing [ArXiv](https://www.kaggle.com/datasets/Cornell-University/arxiv) dataset, which is updated weekly, to create embeddings from computer science paper abstracts. \n\nOur workflow consists of the following steps:\n\n1. âš—ï¸ **Filtering the ArXiv Dataset**: Filter out the enormous ArXiv dataset to pick abstracts from the computer science domain.\n2. â˜‘ï¸ **Selecting Samples for Performance Verification**: Choose specific search queries and their expected paper abstract to verify performance in later steps. \n2. ğŸ—ƒï¸ **Indexing Abstracts**: Use COLBERTv2 to index the filtered abstracts through the Ragatouille library \n3. ğŸ“Š **Verifying Indices**: Evaluate the accuracy of the generated indices using the selected sample search queries.\n\nAll right, lets start!","metadata":{}},{"cell_type":"markdown","source":"### Installing the libraries","metadata":{}},{"cell_type":"code","source":"# Installing ragatouille\n!pip install -qq ragatouille\n# Uninstalling 'faiss-cpu' package and installing 'faiss-gpu' package\n!pip uninstall --y --q faiss-cpu & pip install --q faiss-gpu ","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:04:14.379711Z","iopub.execute_input":"2024-08-15T04:04:14.380052Z","iopub.status.idle":"2024-08-15T04:05:15.625554Z","shell.execute_reply.started":"2024-08-15T04:04:14.380021Z","shell.execute_reply":"2024-08-15T04:05:15.624399Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.2 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Error parsing requirements for faiss-cpu: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/faiss_cpu-1.8.0.post1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## âš—ï¸ Filtering the ArXiv Dataset","metadata":{}},{"cell_type":"markdown","source":"In this section, we filter out the ArXiv dataset to pick abstracts from the computer science domain only. \n\nTo be more specific, we will filter abstracts from the following categories:\n\n- cs.CV (Computer Vision and Pattern Recognition)\n- cs.LG (Machine Learning)\n- cs.CL (Computation and Language)\n- cs.AI (Artificial Intelligence)\n- cs.NE (Neural and Evolutionary Computing)\n- cs.RO (Robotics)\n\nTo do that, we would need to load the JSON dataset into memory using Polars. While you can use Pandas for this task, Polars is often faster for certain operations, making it our choice for filtering the dataset.\n\nLoading the entire dataset occupies approximately 27 GB of RAM. If this RAM is not freed, it can lead to an out-of-memory (OOM) error once we proceed with the next step of indexing. To prevent this, we create a smaller, filtered-out dataset from the large one and save it to disk. To that purpose, we write a script that would perform the filtering operation.\n\nUsing a script clears out the occupied RAM once the script execution is complete. This is very useful for our purposes, as it lets us avoid potential OOM errors.\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/df_build.py\nimport polars as pl\nimport time\nimport argparse\n\n# Function to parse command-line arguments\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Process ArXiv JSON dataset.\")\n    parser.add_argument(\"--input_json\", type=str, default = \"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\", help=\"Path to the input NDJSON file.\")\n    parser.add_argument(\"--output_json\", type=str, default = \"/kaggle/working/arxiv_cs.json\", help=\"Path to the output NDJSON file.\")\n    return parser.parse_args()\n\n# Function to convert the latest 'created' date from the 'versions' field to a specific format\ndef get_latest_time(element):\n    return time.strftime(\"%d %b %Y\", time.strptime(element[-1]['created'], \"%a, %d %b %Y %H:%M:%S %Z\"))\n\n# Function to convert the 'update_date' field to a specific format\ndef get_latest_date(element):\n    return time.strftime(\"%d %b %Y\", time.strptime(element, \"%Y-%m-%d\"))\n\ndef main():\n    args = parse_args()\n\n    # Loading the entire JSON dataset from the input file path\n    cs_arxiv_df = pl.read_ndjson(args.input_json)\n\n    # Filtering rows where the 'categories' column contains specific computer science categories\n    cs_arxiv_df = cs_arxiv_df.filter(pl.col(\"categories\").str.contains(r\"\\b(?:cs\\.(?:CV|LG|CL|AI|NE|RO))\\b\", strict=True))\n\n    # Initializing a new column '_time' with default value 0\n    cs_arxiv_df = cs_arxiv_df.with_columns(pl.lit(0, dtype=pl.Int64).alias('_time'))\n\n    # Updating the '_time' column with the latest version date or update date\n    cs_arxiv_df = cs_arxiv_df.with_columns(\n        pl.when(cs_arxiv_df['versions'].is_not_null())\n        .then(cs_arxiv_df['versions'].map_elements(get_latest_time,  return_dtype=pl.Utf8))\n        .otherwise(cs_arxiv_df['update_date'].map_elements(get_latest_date,  return_dtype=pl.Utf8))\n        .alias('_time')\n    )\n\n    # Columns to be dropped from the DataFrame\n    columns_to_drop = ['versions', 'authors_parsed', 'report-no', 'license', 'submitter']\n\n    # Dropping the specified columns based on the DataFrame type\n    cs_arxiv_df = cs_arxiv_df.drop(columns_to_drop)\n\n    # Writing the processed DataFrame to a new NDJSON file specified by the output path\n    cs_arxiv_df.write_ndjson(args.output_json)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:05:24.260304Z","iopub.execute_input":"2024-08-15T04:05:24.260966Z","iopub.status.idle":"2024-08-15T04:05:24.269086Z","shell.execute_reply.started":"2024-08-15T04:05:24.260931Z","shell.execute_reply":"2024-08-15T04:05:24.268046Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Writing /kaggle/working/df_build.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Time to execute the script! Feel free to have a look at the RAM during execution, and observe how the RAM frees once the script execution is done.","metadata":{}},{"cell_type":"code","source":"# Run the df_build.py script\n!python df_build.py --input_json \"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\" --output_json \"/kaggle/working/arxiv_cs.json\"","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:05:25.058383Z","iopub.execute_input":"2024-08-15T04:05:25.058736Z","iopub.status.idle":"2024-08-15T04:06:29.597069Z","shell.execute_reply.started":"2024-08-15T04:05:25.058709Z","shell.execute_reply":"2024-08-15T04:06:29.595733Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## â˜‘ï¸ Selecting Samples for Performance Verification","metadata":{}},{"cell_type":"markdown","source":"To ensure our RAG system functions correctly and there are no mismatches in the indices or information, we will create a set of search queries, and select their expected results. These handpicked queries will serve as a small benchmark to verify the performance of our system once it is complete.","metadata":{}},{"cell_type":"code","source":"# Load the small JSON dataset consisting of ArXiv CS abstracts\nimport polars as pl\ncs_arxiv = pl.read_ndjson('/kaggle/working/arxiv_cs.json')\ncs_arxiv.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:06:38.299625Z","iopub.execute_input":"2024-08-15T04:06:38.300054Z","iopub.status.idle":"2024-08-15T04:06:39.342197Z","shell.execute_reply.started":"2024-08-15T04:06:38.300018Z","shell.execute_reply":"2024-08-15T04:06:39.341308Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"shape: (2, 10)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id        â”† authors   â”† title     â”† comments  â”† â€¦ â”† categorie â”† abstract  â”† update_da â”† _time    â”‚\nâ”‚ ---       â”† ---       â”† ---       â”† ---       â”†   â”† s         â”† ---       â”† te        â”† ---      â”‚\nâ”‚ str       â”† str       â”† str       â”† str       â”†   â”† ---       â”† str       â”† ---       â”† str      â”‚\nâ”‚           â”†           â”†           â”†           â”†   â”† str       â”†           â”† str       â”†          â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 0704.0047 â”† T. Kosel  â”† Intellige â”† 5 pages,  â”† â€¦ â”† cs.NE     â”† The intel â”† 2009-09-2 â”† 01 Apr   â”‚\nâ”‚           â”† and I.    â”† nt        â”† 5 eps     â”†   â”† cs.AI     â”† ligent    â”† 9         â”† 2007     â”‚\nâ”‚           â”† Grabec    â”† location  â”† figures,  â”†   â”†           â”† acoustic  â”†           â”†          â”‚\nâ”‚           â”†           â”† of        â”† uses Iâ€¦   â”†   â”†           â”† emiâ€¦      â”†           â”†          â”‚\nâ”‚           â”†           â”† simultâ€¦   â”†           â”†   â”†           â”†           â”†           â”†          â”‚\nâ”‚ 0704.0050 â”† T. Kosel  â”† Intellige â”† 5 pages,  â”† â€¦ â”† cs.NE     â”† Part I    â”† 2007-05-2 â”† 01 Apr   â”‚\nâ”‚           â”† and I.    â”† nt        â”† 7 eps     â”†   â”† cs.AI     â”† describes â”† 3         â”† 2007     â”‚\nâ”‚           â”† Grabec    â”† location  â”† figures,  â”†   â”†           â”† an        â”†           â”†          â”‚\nâ”‚           â”†           â”† of        â”† uses Iâ€¦   â”†   â”†           â”† intelligâ€¦ â”†           â”†          â”‚\nâ”‚           â”†           â”† simultâ€¦   â”†           â”†   â”†           â”†           â”†           â”†          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (2, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>authors</th><th>title</th><th>comments</th><th>journal-ref</th><th>doi</th><th>categories</th><th>abstract</th><th>update_date</th><th>_time</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;0704.0047&quot;</td><td>&quot;T. Kosel and I. Grabec&quot;</td><td>&quot;Intelligent location of simultâ€¦</td><td>&quot;5 pages, 5 eps figures, uses Iâ€¦</td><td>null</td><td>null</td><td>&quot;cs.NE cs.AI&quot;</td><td>&quot;  The intelligent acoustic emiâ€¦</td><td>&quot;2009-09-29&quot;</td><td>&quot;01 Apr 2007&quot;</td></tr><tr><td>&quot;0704.0050&quot;</td><td>&quot;T. Kosel and I. Grabec&quot;</td><td>&quot;Intelligent location of simultâ€¦</td><td>&quot;5 pages, 7 eps figures, uses Iâ€¦</td><td>null</td><td>null</td><td>&quot;cs.NE cs.AI&quot;</td><td>&quot;  Part I describes an intelligâ€¦</td><td>&quot;2007-05-23&quot;</td><td>&quot;01 Apr 2007&quot;</td></tr></tbody></table></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Here, we pick the search queries `mistral`,`mixtral`, `gemini`, `ColBERTv2` and the DOI of their respective papers (`2310.06825`, `2401.04088`, `2312.11805`, `2112.01488` )","metadata":{}},{"cell_type":"code","source":"search_queries = {'mistral':'2310.06825',\n                  'mixtral': '2401.04088',\n                  'gemini': '2312.11805',\n                  'ColBERTv2': '2112.01488',}\nsearch_ids = list(search_queries.values())\n## Get the metadata from the DOI/search_ids in a dataframe\nfiltered_df = cs_arxiv.filter(pl.col('id').is_in(search_ids))","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:06:45.142498Z","iopub.execute_input":"2024-08-15T04:06:45.143131Z","iopub.status.idle":"2024-08-15T04:06:45.157842Z","shell.execute_reply.started":"2024-08-15T04:06:45.143103Z","shell.execute_reply":"2024-08-15T04:06:45.157025Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Let's save this dataset in a JSON file","metadata":{}},{"cell_type":"code","source":"filtered_df.write_ndjson('/kaggle/working/arxiv_cs_verify.json')","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:06:46.842996Z","iopub.execute_input":"2024-08-15T04:06:46.843421Z","iopub.status.idle":"2024-08-15T04:06:46.848142Z","shell.execute_reply.started":"2024-08-15T04:06:46.843390Z","shell.execute_reply":"2024-08-15T04:06:46.847119Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ—ƒï¸ Indexing Abstracts","metadata":{}},{"cell_type":"code","source":"# Create a directory where we can store the data\nimport os\nupload_folder = 'arxiv'\nupload_dir = os.path.join('/kaggle/working/', upload_folder)\nos.makedirs(upload_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:06:48.668733Z","iopub.execute_input":"2024-08-15T04:06:48.669451Z","iopub.status.idle":"2024-08-15T04:06:48.673857Z","shell.execute_reply.started":"2024-08-15T04:06:48.669421Z","shell.execute_reply":"2024-08-15T04:06:48.672905Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Picking the metadata","metadata":{}},{"cell_type":"markdown","source":"Here we would be picking which metadata we want to store along with the abstracts. While not all metadata might be useful for our case, we would store the DOI, author names, paper title, and the time the paper last updated (denoted by `doi`, `authors`, `title` and `_time` respectively) along with the abstracts.","metadata":{}},{"cell_type":"code","source":"abstracts = list(cs_arxiv['abstract'])\nids = list(cs_arxiv['id'])\nmetadata = cs_arxiv[['doi','authors','title', '_time']]\n\n# convert metadata to string to avoid Nonetype error \nif isinstance(metadata, pl.DataFrame):\n    metadata = metadata.fill_null('None')\n    metadata = metadata.to_dicts()\nelse:\n    raise Exception","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:06:50.596184Z","iopub.execute_input":"2024-08-15T04:06:50.596924Z","iopub.status.idle":"2024-08-15T04:06:51.934949Z","shell.execute_reply.started":"2024-08-15T04:06:50.596893Z","shell.execute_reply":"2024-08-15T04:06:51.934002Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Loading Pre-trained ColBERTv2 and Indexing Abstracts","metadata":{}},{"cell_type":"markdown","source":"First, we load the pretrained ColBERTv2 model using the Ragatouille library.","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom ragatouille import RAGPretrainedModel\n# We load the pretrained model \"colbert-ir/colbertv2.0\" and specify a location to store the index\nColbertv2 = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\", index_root= upload_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:06:53.426881Z","iopub.execute_input":"2024-08-15T04:06:53.427202Z","iopub.status.idle":"2024-08-15T04:07:12.199006Z","shell.execute_reply.started":"2024-08-15T04:06:53.427178Z","shell.execute_reply":"2024-08-15T04:07:12.198058Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c68339d6fc494db847ec9f578edc6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108063ab612a4f81bfb131a3f1cc8d4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d949b8c38e74b03a60a742beb8d4f85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0734f48065a4bba800838b14af4c41a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb34ee1c03f043a385d4935a1d3bf768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c53f8afc4c844a5985f50bb96b055ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77046c63a5241d5a7dfb3a1f1bb2e11"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 9.87 s, sys: 2.48 s, total: 12.3 s\nWall time: 18.8 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now this is what we have been working for - time to start indexing!","metadata":{}},{"cell_type":"code","source":"%%time\nColbertv2.index(collection = abstracts, \n        document_ids = ids,\n        document_metadatas = metadata,\n        index_name = 'arxiv_colbert',\n        max_document_length = 512,\n        split_documents = False,\n        bsize = 128\n        )","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:07:16.423790Z","iopub.execute_input":"2024-08-15T04:07:16.424393Z","iopub.status.idle":"2024-08-15T04:07:46.272288Z","shell.execute_reply.started":"2024-08-15T04:07:16.424365Z","shell.execute_reply":"2024-08-15T04:07:46.271436Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\n\n[Aug 15, 04:07:18] #> Creating directory /kaggle/working/arxiv/colbert/indexes/arxiv_colbert \n\n\n[Aug 15, 04:07:21] [0] \t\t #> Encoding 108609 passages..\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed eval>:1\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragatouille/RAGPretrainedModel.py:211\u001b[0m, in \u001b[0;36mRAGPretrainedModel.index\u001b[0;34m(self, collection, document_ids, document_metadatas, index_name, overwrite_index, max_document_length, split_documents, document_splitter_fn, preprocessing_fn, bsize, use_faiss)\u001b[0m\n\u001b[1;32m    202\u001b[0m     document_splitter_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    203\u001b[0m collection, pid_docid_map, docid_metadata_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_corpus(\n\u001b[1;32m    204\u001b[0m     collection,\n\u001b[1;32m    205\u001b[0m     document_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     max_document_length,\n\u001b[1;32m    210\u001b[0m )\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpid_docid_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid_docid_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocid_metadata_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocid_metadata_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_document_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_document_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_faiss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_faiss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragatouille/models/colbert.py:341\u001b[0m, in \u001b[0;36mColBERT.index\u001b[0;34m(self, collection, pid_docid_map, docid_metadata_map, index_name, max_document_length, overwrite, bsize, use_faiss)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocid_pid_map[docid]\u001b[38;5;241m.\u001b[39mappend(pid)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocid_metadata_map \u001b[38;5;241m=\u001b[39m docid_metadata_map\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index \u001b[38;5;241m=\u001b[39m \u001b[43mModelIndexFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPLAID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_faiss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_faiss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_index_metadata()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragatouille/models/index.py:485\u001b[0m, in \u001b[0;36mModelIndexFactory.construct\u001b[0;34m(index_type, config, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;66;03m# NOTE: For now only PLAID indexes are supported.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     index_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModelIndexFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_MODEL_INDEX_BY_NAME\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mModelIndexFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_invalid_index_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragatouille/models/index.py:150\u001b[0m, in \u001b[0;36mPLAIDModelIndex.construct\u001b[0;34m(config, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct\u001b[39m(\n\u001b[1;32m    142\u001b[0m     config: ColBERTConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAIDModelIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPLAIDModelIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragatouille/models/index.py:254\u001b[0m, in \u001b[0;36mPLAIDModelIndex.build\u001b[0;34m(self, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m Indexer(\n\u001b[1;32m    249\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint,\n\u001b[1;32m    250\u001b[0m         config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    251\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    253\u001b[0m     indexer\u001b[38;5;241m.\u001b[39mconfigure(avoid_fork_if_possible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/indexer.py:80\u001b[0m, in \u001b[0;36mIndexer.index\u001b[0;34m(self, name, collection, overwrite)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merase()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_does_not_exist \u001b[38;5;129;01mor\u001b[39;00m overwrite \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreuse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_path\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/indexer.py:89\u001b[0m, in \u001b[0;36mIndexer.__launch\u001b[0;34m(self, collection)\u001b[0m\n\u001b[1;32m     87\u001b[0m     shared_queues \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     88\u001b[0m     shared_lists \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_without_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_queues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     93\u001b[0m manager \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mManager()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/infra/launcher.py:93\u001b[0m, in \u001b[0;36mLauncher.launch_without_fork\u001b[0;34m(self, custom_config, *args)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (custom_config\u001b[38;5;241m.\u001b[39mavoid_fork_if_possible \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config\u001b[38;5;241m.\u001b[39mavoid_fork_if_possible)\n\u001b[1;32m     92\u001b[0m new_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(custom_config)\u001b[38;5;241m.\u001b[39mfrom_existing(custom_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config, RunConfig(rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 93\u001b[0m return_val \u001b[38;5;241m=\u001b[39m \u001b[43mrun_process_without_mp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallee\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_val\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/infra/launcher.py:109\u001b[0m, in \u001b[0;36mrun_process_without_mp\u001b[0;34m(callee, config, *args)\u001b[0m\n\u001b[1;32m    106\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, config\u001b[38;5;241m.\u001b[39mgpus_[:config\u001b[38;5;241m.\u001b[39mnranks]))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Run()\u001b[38;5;241m.\u001b[39mcontext(config, inherit_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 109\u001b[0m     return_val \u001b[38;5;241m=\u001b[39m \u001b[43mcallee\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_val\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py:33\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(config, collection, shared_lists, shared_queues, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(config, collection, shared_lists, shared_queues, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     32\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m CollectionIndexer(config\u001b[38;5;241m=\u001b[39mconfig, collection\u001b[38;5;241m=\u001b[39mcollection, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_lists\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py:63\u001b[0m, in \u001b[0;36mCollectionIndexer.run\u001b[0;34m(self, shared_lists)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, shared_lists):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 63\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Computes and saves plan for whole collection\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         distributed\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank)\n\u001b[1;32m     65\u001b[0m         print_memory_stats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py:101\u001b[0m, in \u001b[0;36mCollectionIndexer.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Saves sampled passages and embeddings for training k-means centroids later \u001b[39;00m\n\u001b[1;32m    100\u001b[0m sampled_pids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_pids()\n\u001b[0;32m--> 101\u001b[0m avg_doclen_est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_pids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Select the number of partitions\u001b[39;00m\n\u001b[1;32m    104\u001b[0m num_passages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py:137\u001b[0m, in \u001b[0;36mCollectionIndexer._sample_embeddings\u001b[0;34m(self, sampled_pids)\u001b[0m\n\u001b[1;32m    134\u001b[0m local_pids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39menumerate(rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank)\n\u001b[1;32m    135\u001b[0m local_sample \u001b[38;5;241m=\u001b[39m [passage \u001b[38;5;28;01mfor\u001b[39;00m pid, passage \u001b[38;5;129;01min\u001b[39;00m local_pids \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m sampled_pids]\n\u001b[0;32m--> 137\u001b[0m local_sample_embs, doclens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_passages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/indexing/collection_encoder.py:26\u001b[0m, in \u001b[0;36mCollectionEncoder.encode_passages\u001b[0;34m(self, passages)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Batch here to avoid OOM from storing intermediate embeddings on GPU.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Storing on the GPU helps with speed of masking, etc.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# But ideally this batching happens internally inside docFromText.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m passages_batch \u001b[38;5;129;01min\u001b[39;00m batch(passages, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mindex_bsize \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     embs_, doclens_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocFromText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassages_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_bsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mkeep_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflatten\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshowprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     embs\u001b[38;5;241m.\u001b[39mappend(embs_)\n\u001b[1;32m     29\u001b[0m     doclens\u001b[38;5;241m.\u001b[39mextend(doclens_)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/modeling/checkpoint.py:67\u001b[0m, in \u001b[0;36mCheckpoint.docFromText\u001b[0;34m(self, docs, bsize, keep_dims, to_cpu, showprogress, return_tokens)\u001b[0m\n\u001b[1;32m     64\u001b[0m     returned_text \u001b[38;5;241m=\u001b[39m [returned_text]\n\u001b[1;32m     66\u001b[0m keep_dims_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_dims \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatten\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m keep_dims\n\u001b[0;32m---> 67\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc(input_ids, attention_mask, keep_dims\u001b[38;5;241m=\u001b[39mkeep_dims_, to_cpu\u001b[38;5;241m=\u001b[39mto_cpu)\n\u001b[1;32m     68\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask \u001b[38;5;129;01min\u001b[39;00m tqdm(text_batches, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m showprogress)]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     D \u001b[38;5;241m=\u001b[39m _stack_3D_tensors(batches)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/modeling/checkpoint.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m     returned_text \u001b[38;5;241m=\u001b[39m [returned_text]\n\u001b[1;32m     66\u001b[0m keep_dims_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_dims \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatten\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m keep_dims\n\u001b[0;32m---> 67\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_dims_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_cpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask \u001b[38;5;129;01min\u001b[39;00m tqdm(text_batches, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m showprogress)]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     D \u001b[38;5;241m=\u001b[39m _stack_3D_tensors(batches)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/modeling/checkpoint.py:38\u001b[0m, in \u001b[0;36mCheckpoint.doc\u001b[0;34m(self, to_cpu, *args, **kw_args)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_manager\u001b[38;5;241m.\u001b[39mcontext():\n\u001b[0;32m---> 38\u001b[0m         D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m to_cpu:\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (D[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), \u001b[38;5;241m*\u001b[39mD[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(D, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m D\u001b[38;5;241m.\u001b[39mcpu()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/modeling/colbert.py:101\u001b[0m, in \u001b[0;36mColBERT.doc\u001b[0;34m(self, input_ids, attention_mask, keep_dims)\u001b[0m\n\u001b[1;32m     99\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    100\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(D)\n\u001b[0;32m--> 101\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskiplist\u001b[49m\u001b[43m)\u001b[49m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    102\u001b[0m D \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m*\u001b[39m mask\n\u001b[1;32m    104\u001b[0m D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(D, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/colbert/modeling/colbert.py:125\u001b[0m, in \u001b[0;36mColBERT.mask\u001b[0;34m(self, input_ids, skiplist)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, skiplist):\n\u001b[0;32m--> 125\u001b[0m     mask \u001b[38;5;241m=\u001b[39m [[(x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skiplist) \u001b[38;5;129;01mand\u001b[39;00m (x \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m d] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## ğŸ“Š Verifying Indices","metadata":{}},{"cell_type":"markdown","source":"All right! Now that the embeddings have been generated, lets check the retrieval performance. We would use the previously handpicked queries and see if our expected result is within the top 10 queries or not. We would also match the retrieved result content (abstract, title) with the expected content ","metadata":{}},{"cell_type":"code","source":"Colbertv2.search(query=\"What is mistral\", k=10)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T03:43:13.880576Z","iopub.status.idle":"2024-08-15T03:43:13.880992Z","shell.execute_reply.started":"2024-08-15T03:43:13.880789Z","shell.execute_reply":"2024-08-15T03:43:13.880806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To verify the result, we would be searching using the predefined search queries (`mistral`,`mixtral`, `gemini`, `ColBERTv2` in this case), and see if any of the top-10 search results contain the expected DOI (`document_id` in the `rag_output`).\n\nIf the `document_id` matches with the DOI, we would do an additional verification - we would check if the metadata stored accurately matches with the expected metadata. There can be accidental cases when the metadata of all the indices gets mismatched, this would be a check to ensure it did not happen.","metadata":{}},{"cell_type":"code","source":"# Initialize counters for total number of queries and number of wrong results\ntotal = 0\nwrong = 0\n\n# Loop through each query and its corresponding value from the search_queries dictionary\nfor query, value in search_queries.items():\n    # Perform a search using the query and retrieve the top 10 results\n    rag_output = Colbertv2.search(query=query, k=10)\n    \n    # Retrieve the ground truth data based on the value from filtered_df\n    gnd_truth = filtered_df.row(by_predicate=(pl.col(\"id\") == value), named=True)\n    \n    # Initialize a flag to check if there is a match\n    match = False\n    \n    # Loop through the search results to check if any match the ground truth\n    for rag_output_val in rag_output:\n        # Check if the document ID matches the ground truth ID\n        if rag_output_val['document_id'] == gnd_truth['id']:\n            # Assert that the titles are the same\n            assert rag_output_val['document_metadata']['title'] == gnd_truth['title']\n            \n            # Check if the content matches the ground truth abstract\n            abs_match_1 = rag_output_val['content'] == gnd_truth['abstract']\n            abs_match_2 = rag_output_val['content'].replace('\\n', '').replace(' ', '') == gnd_truth['abstract'].replace('\\n', '').replace(' ', '')\n            \n            # Assert that either of the two abstract match conditions is true\n            assert any([abs_match_1, abs_match_2])\n            \n            # Set the match flag to True\n            match = True\n    \n    # If no match was found, increment the wrong counter and print the ground truth and search results\n    if not match:\n        wrong += 1\n        print(gnd_truth)\n        print(rag_output)\n        \n    # Increment the total counter for each query\n    total += 1\n\n# Print the number of correct matches and the accuracy percentage\nprint(\"Matches : \", total - wrong, \" Accuracy:\", round((total - wrong) * 100 / total, 2))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-15T03:43:13.883176Z","iopub.status.idle":"2024-08-15T03:43:13.883613Z","shell.execute_reply.started":"2024-08-15T03:43:13.883410Z","shell.execute_reply":"2024-08-15T03:43:13.883428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Awesome! If things look great, we can use these indices in our [ArXiv CS RAG](https://huggingface.co/spaces/bishmoy/Arxiv-CS-RAG) space.","metadata":{}}]}